---
# =============================================================================
# MAIN ASSESSMENT FLOW CONTROLLER
# =============================================================================
# Purpose: Orchestrate the complete assessment flow from start to finish
# Input: User information, questions, pain points, AI processing
# Output: Complete assessment results with AI insights and visualizations
# Usage: Main flow controller that manages all assessment phases
# Dependencies: All other modules (questions, scoring, AI, visualization)

id: main_assessment_flow
mandatory: True
code: |
  """
  MAIN ASSESSMENT FLOW CONTROLLER - Day 6 Complete System
  
  This is the central orchestrator for the entire assessment system. It manages:
  1. User information collection and validation
  2. Pain points prioritization
  3. Question flow and category feedback
  4. Score calculation and processing
  5. AI background processing and result merging
  6. Results display and document generation
  
  Flow Phases:
  - 'questions': User answers 25 assessment questions (number to be set by csv file)
  - 'processing': AI generates insights in background
  - 'results': Display final results with visualizations
  
  State Management:
  - Tracks progress through multiple completion flags
  - Handles user navigation and auto-advancement
  - Manages background AI task lifecycle
  - Ensures data consistency across all phases
  """
  
  # MAIN ASSESSMENT FLOW - Day 6 Complete System
  # This runs AFTER user information is collected
  
  # =============================================================================
  # INITIALIZATION AND CONFIGURATION
  # =============================================================================
  
  # Ensure level mapping configuration is loaded for question formatting
  need('formatting_config')
  
  # FIRST: Ensure user information is collected (this will trigger the mandatory user question)
  if not defined('user_info_collected') or not user_info_collected:
    # Don't proceed until user info is collected
    log("‚è∏Ô∏è Waiting for user information collection...")
    # This will stop execution here and wait for user_info_collected to be True
    user_info_collected

  # Provide a safe default for AI processing status to avoid dependency loops
  if not defined('ai_processing_complete'):
    ai_processing_complete = False

  # Do not predefine feedback_provided; it must be undefined to trigger the feedback question when needed

  # Unified proceed flag used across flow control - checks multiple navigation signals
  any_proceed = (defined('proceeded_to_results') and proceeded_to_results) or \
                (defined('proceed_to_results') and proceed_to_results)

  # =============================================================================
  # NAVIGATION SHORT-CIRCUIT HANDLING
  # =============================================================================
  # Purpose: Handle user navigation to results, bypassing intermediate steps
  # Input: Navigation flags from UI (proceeded_to_results, proceed_to_results)
  # Output: Clean state transition to results phase
  # Usage: Prevents detours into questions/processing when user wants results
  
  # HARD SHORT-CIRCUIT: if user has proceeded to results, clear any pending
  # question/feedback state and immediately show results. This prevents detours
  # into category feedback or re-entry into questions/processing.
  if (defined('proceeded_to_results') and proceeded_to_results) or (defined('proceeded_to_results') and proceed_to_results):
    try:
      # Clear any pending category feedback state
      if defined('need_category_feedback'):
        del need_category_feedback
      feedback_provided = True
      if defined('category_feedback'):
        category_feedback = {}
    except Exception:
      pass
    try:
      # Mark questions as complete to prevent re-entry
      questions_processed = True
      questions_complete = True
      if defined('questions'):
        current_question = len(questions)
      if defined('current_question_response'):
        del current_question_response
    except Exception:
      pass
    # Set phase to results and navigate immediately
    ready_for_processing = False
    processing_started = False
    phase = 'results'
    need('show_assessment_results')

  # AUTO-ADVANCE SAFETY: if the UI signaled completion, finalize proceed
  if (defined('ui_progress_done') and ui_progress_done) and not (defined('proceeded_to_results') and proceeded_to_results):
    try:
      # Auto-advance to results when UI signals completion
      proceeded_to_results = True
      processing_complete = True
      phase = 'results'
      ready_for_processing = False
      processing_started = False
      if defined('need_category_feedback'):
        del need_category_feedback
      feedback_provided = True
      log('‚úÖ Auto-advanced from UI completion signal')
    except Exception:
      pass
    need('show_assessment_results')

  # If user has explicitly proceeded past processing, go straight to results and skip any re-entry
  # Do NOT auto-jump on processing_complete; user must click Continue on the progress screen
  if (defined('proceeded_to_results') and proceeded_to_results) or (defined('proceed_to_results') and proceed_to_results):
    if not defined('post_processing_cleanup_done'):
      try:
        # Clean up processing state to prevent re-entry
        ready_for_processing = False
        processing_started = False
        processing_complete = True
        post_processing_cleanup_done = True
        if defined('need_category_feedback'):
          del need_category_feedback
        feedback_provided = True
        log("‚û°Ô∏è Skipping processing re-entry; going straight to results")
      except Exception:
        pass
    # Immediately navigate to results screen
    need('show_assessment_results')

  if (defined('proceeded_to_results') and proceeded_to_results) or (defined('proceed_to_results') and proceed_to_results):
    pass
  # SECOND: Create user_information object if it doesn't exist
  if not defined('user_information') or not user_information:
    if defined('user_first_name') and user_first_name:
      user_information = DADict('user_information', auto_gather=False)
      user_information['first_name'] = user_first_name
      user_information['last_name'] = user_last_name if defined('user_last_name') else ''
      user_information['surname'] = user_last_name if defined('user_last_name') else ''
      user_information['company'] = user_company if defined('user_company') else ''
      user_information['role'] = user_role if defined('user_role') else ''
      user_information['email'] = user_email if defined('user_email') else ''
      user_information['industry'] = user_industry if defined('user_industry') else ''
      user_information.gathered = True
      
      # Also set as object attributes for template compatibility
      user_information.first_name = user_first_name
      user_information.last_name = user_last_name if defined('user_last_name') else ''
      user_information.surname = user_last_name if defined('user_last_name') else ''
      user_information.company = user_company if defined('user_company') else ''
      user_information.role = user_role if defined('user_role') else ''
      user_information.email = user_email if defined('user_email') else ''
      user_information.industry = user_industry if defined('user_industry') else ''
      
      log(f"‚úÖ Created user_information: {user_first_name} at {user_company}")
    else:
      log("‚ùå ERROR: User information variables not available!")
      # This should not happen if user_info_collected is True
      user_info_collected = False  # Reset to trigger collection again
  
  # =============================================================================
  # ASSESSMENT FLOW INITIALIZATION
  # =============================================================================
  # Purpose: Initialize the assessment flow and set up initial state
  # Input: Navigation flags and user state
  # Output: Initial phase setting and debug tracking setup
  # Usage: One-time initialization when assessment flow starts
  
  if not defined('assessment_flow_started'):
    log("üöÄ Starting Day 6 Complete Assessment System...")
    assessment_flow_started = True
    
    # Set initial phase based on user state
    if not defined('phase'):
      phase = 'questions'
    # If the user already clicked proceed, lock phase immediately
    if defined('proceeded_to_results') and proceeded_to_results:
      phase = 'results'
    # Do not reset navigation flags here; they are used to advance to results after processing
    
    # Initialize debug tracking for troubleshooting
    if not defined('debug_info'):
      debug_info = []
    
    debug_info.append({
      'stage': 'initialization',
      'timestamp': str(today()),
      'data': 'Assessment system started'
    })
  
  # =============================================================================
  # DATA LOADING AND PAIN POINTS COLLECTION
  # =============================================================================
  # Purpose: Load all required data and collect user pain point priorities
  # Input: CSV data files, user pain point selections
  # Output: Loaded questions, pain points, and offering documents
  # Usage: Step 1-2 of assessment flow - data preparation and user input
  
  # Step 1: Load questions, pain points, and offering documents
  if not defined('data_loaded'):
    need('questions_loaded')
    need('pain_points_loaded')
    need('load_offering_document_content')  # Load offering documents
    data_loaded = True
    log("‚úÖ Data loaded")
  
  # Step 2: Collect pain points
  if not defined('pain_points_processed'):
    collect_pain_points_complete
    pain_points_processed = True
    log("‚úÖ Pain points collected")
    
    # Calculate pain points data immediately after collection
    need('calculate_pain_points_data')
    log("‚úÖ Pain points data calculated")
  
  # =============================================================================
  # QUESTION FLOW MANAGEMENT
  # =============================================================================
  # Purpose: Manage the 25-question assessment flow with category feedback
  # Input: Questions data, user responses, category feedback requirements
  # Output: Completed questions and readiness for processing phase
  # Usage: Step 3 of assessment flow - question presentation and feedback collection
  
  # Step 3: Ask all questions but do NOT force feedback from here; let the questions file handle it
  # Skip re-entering the question flow if we are already ready for processing
  if (not defined('phase') or phase == 'questions') and not (defined('ready_for_processing') and ready_for_processing) and not defined('questions_processed') and not any_proceed:
    # If category feedback is pending, gather it BEFORE proceeding
    if defined('need_category_feedback') and not defined('feedback_provided'):
      feedback_provided
    # Enter the question flow (the questions file will raise the feedback screen when needed)
    questions_complete

    # If feedback is pending, do not mark as processed here; allow the feedback question to render next
    if defined('need_category_feedback') and not defined('feedback_provided'):
      log("‚è≥ Awaiting category feedback; not proceeding to processing yet")
    elif defined('questions_complete') and questions_complete:
      # Do not clear category feedback here; let the feedback screen handle it
      questions_processed = True
      ready_for_processing = True
      phase = 'processing'
      
      # FORCE IMMEDIATE AI LAUNCH when questions complete
      if not defined('ai_background_launched'):
        log("üöÄ IMMEDIATE: Launching AI background task as soon as questions complete")
        try:
          from docassemble.base.util import background_action
          
          # Prepare assessment data for background task
          assessment_data_for_ai = {
            'overall_score': overall_score if defined('overall_score') else defaults.get('overall_score', 0),
            'industry_average': industry_average if defined('industry_average') else industry_averages.get('default', 2.18),
            'category_averages': category_averages if defined('category_averages') else {},
            'category_scores': category_scores if defined('category_scores') else (category_averages if defined('category_averages') else {}),
            'pain_points': pain_points if defined('pain_points') else []
          }
          
          # Launch AI background task for immediate processing
          ai_task = background_action('run_ai_chunks_v2', assessment_data=assessment_data_for_ai)
          ai_background_launched = True
          log("‚úÖ IMMEDIATE: AI background task launched with questions completion")
        except Exception as immediate_err:
          log(f"‚ùå IMMEDIATE: Failed to launch AI background task: {immediate_err}")
      
      log("‚úÖ Questions completed (feedback handled by questions file)")
    # Fallback: if index shows we're done but questions_complete hasn't resolved yet
    elif defined('questions') and defined('current_question') and current_question >= len(questions) and not (defined('need_category_feedback') and not defined('feedback_provided')):
      questions_processed = True
      ready_for_processing = True
      phase = 'processing'
      
      # FORCE IMMEDIATE AI LAUNCH when questions complete by index
      if not defined('ai_background_launched'):
        log("üöÄ IMMEDIATE: Launching AI background task as soon as questions complete (by index)")
        try:
          from docassemble.base.util import background_action
          
          # Prepare assessment data for background task
          assessment_data_for_ai = {
            'overall_score': overall_score if defined('overall_score') else defaults.get('overall_score', 0),
            'industry_average': industry_average if defined('industry_average') else industry_averages.get('default', 2.18),
            'category_averages': category_averages if defined('category_averages') else {},
            'category_scores': category_scores if defined('category_scores') else (category_averages if defined('category_averages') else {}),
            'pain_points': pain_points if defined('pain_points') else []
          }
          
          ai_task = background_action('run_ai_chunks_v2', assessment_data=assessment_data_for_ai)
          ai_background_launched = True
          log("‚úÖ IMMEDIATE: AI background task launched with questions completion (by index)")
        except Exception as immediate_err:
          log(f"‚ùå IMMEDIATE: Failed to launch AI background task: {immediate_err}")
      
      log("‚úÖ Questions completed by index; proceeding to processing")
  
  # =============================================================================
  # PROCESSING PHASE INITIATION
  # =============================================================================
  # Purpose: Start the processing phase with score calculation and AI background task
  # Input: Completed questions, user responses, assessment data
  # Output: Calculated scores, background AI task, processing state
  # Usage: Step 4 of assessment flow - score calculation and AI processing initiation
  
  # Step 4: Start processing IMMEDIATELY when ready (all questions + feedback)
  # Background AI should start as soon as processing phase begins
  log(f"üîç PROCESSING CHECK: phase={phase if defined('phase') else 'undefined'}, ready_for_processing={ready_for_processing if defined('ready_for_processing') else 'undefined'}, any_proceed={any_proceed}")
  log(f"üîç PROCESSING CHECK: processing_started={processing_started if defined('processing_started') else 'undefined'}")
  if (defined('phase') and phase == 'processing') and defined('ready_for_processing') and ready_for_processing:
    log("üöÄ Processing assessment immediately...")
    from docassemble.base.util import background_action
    # FORCE AI background task launch regardless of processing_started state
    log(f"üîç FORCE CHECK: Will launch AI task. processing_started={processing_started if defined('processing_started') else 'undefined'}")
    
    # =============================================================================
    # PROGRESS LOG INITIALIZATION
    # =============================================================================
    
    # Initialize progress log and processing state machine
    try:
      progress_log.clear()
    except Exception:
      pass
    try:
      progress_log.gathered = True
    except Exception:
      pass
    progress_log.append('Initializing processing‚Ä¶')
    
    # =============================================================================
    # SYNCHRONOUS SCORE CALCULATION
    # =============================================================================
    # Purpose: Calculate all scores before launching AI background task
    # Input: User answers, questions data, level mapping
    # Output: Category averages, overall score, individual scores
    # Usage: Pre-processing score calculation for immediate availability
    
    # Do all synchronous/lightweight work BEFORE background AI
    try:
      progress_log.append('Preparing scores‚Ä¶')
      
      # CRITICAL: Directly call score calculation functions instead of using need()
      log("üîç FORCE: Directly calling score calculation functions...")
      
      # Load configuration first to ensure all constants are available
      need('load_configuration_constants')
      
      # Call the score calculation functions directly
      from docassemble.base.util import get_config
      
      # =============================================================================
      # CATEGORY AVERAGES CALCULATION
      # =============================================================================
      # Purpose: Calculate category-specific scores from user answers
      # Input: User answers, questions with categories, level mapping
      # Output: category_averages dictionary with scores per category
      # Usage: Provides category-level performance metrics
      
      # Calculate category averages
      log("üîç FORCE: Calculating category averages...")
      category_averages = {}
      category_totals = {}
      category_counts = {}
      
      # Process each answer to calculate category averages
      for i, answer in enumerate(answers, 1):
        if answer and ':' in answer:
          # Extract the level using centralized level mapping
          level_letter = answer.split(':')[0].strip()
          level = level_mapping.get(level_letter, 0)
          
          # Get category for this question
          category = questions[i-1]['category'] if i <= len(questions) else 'Unknown'
          
          # Initialize category totals if not exists
          if category not in category_totals:
            category_totals[category] = 0
            category_counts[category] = 0
          
          # Accumulate scores for category average calculation
          category_totals[category] += level
          category_counts[category] += 1
      
      # Calculate final category averages
      for category in category_totals:
        if category_counts[category] > 0:
          category_averages[category] = category_totals[category] / category_counts[category]
      
      log(f"üîç FORCE: Category averages calculated: {category_averages}")
      
      # =============================================================================
      # OVERALL SCORE AND INDUSTRY AVERAGE CALCULATION
      # =============================================================================
      # Purpose: Calculate overall assessment score and set industry benchmark
      # Input: category_averages, industry_averages configuration
      # Output: overall_score, industry_average for comparison
      # Usage: Provides overall performance metric and industry benchmark
      
      # Calculate overall score
      log("üîç FORCE: Calculating overall score...")
      if category_averages:
        overall_score = sum(category_averages.values()) / len(category_averages)
      else:
        overall_score = 0
      
      log(f"üîç FORCE: Overall score calculated: {overall_score}")
      
      # Set industry average from configuration
      industry_average = industry_averages.get('default', 2.18)
      
      # =============================================================================
      # INDIVIDUAL SCORES GENERATION
      # =============================================================================
      # Purpose: Generate question-level scores for visualization plots
      # Input: User answers, questions with industry norms, level mapping
      # Output: individual_scores list for lollipop plot generation
      # Usage: Provides data for question-level performance visualization
      
      # Generate individual scores for plots
      log("üîç FORCE: Generating individual scores...")
      individual_scores = []
      for i, answer in enumerate(answers, 1):
        if answer and ':' in answer:
          # Extract user level from answer
          level_letter = answer.split(':')[0].strip()
          level = level_mapping.get(level_letter, 0)
          
          # Get industry norm score from the question - USE ACTUAL CSV DATA
          industry_level = 2.0  # Fallback only
          if defined('questions') and questions and i-1 < len(questions):
            question = questions[i-1]
            if 'IndustryNormScore' in question:
              industry_level = float(question['IndustryNormScore'])
            else:
              log(f"‚ö†Ô∏è FLOW: Missing IndustryNormScore for Q{i}")
          
          # Create dictionary format for lollipop plot compatibility
          individual_scores.append({
            'question_label': f'Q{i}',
            'user_level': level,
            'industry_level': industry_level
          })
        else:
          # Handle unanswered questions with fallback values
          industry_level = 2.0  # Fallback only
          if defined('questions') and questions and i-1 < len(questions):
            question = questions[i-1]
            if 'IndustryNormScore' in question:
              industry_level = float(question['IndustryNormScore'])
            else:
              log(f"‚ö†Ô∏è FLOW: Missing IndustryNormScore for Q{i}")
          
          individual_scores.append({
            'question_label': f'Q{i}',
            'user_level': 0,
            'industry_level': industry_level
          })
      
      # =============================================================================
      # VARIABLE DEFINITION AND VISUALIZATION GENERATION
      # =============================================================================
      # Purpose: Define calculated variables and generate visualization plots
      # Input: Calculated scores, individual scores data
      # Output: Defined variables, lollipop plot, assessment date
      # Usage: Makes scores available throughout system and creates visualizations
      
      # Define all the calculated variables
      define('category_averages', category_averages)
      define('overall_score', overall_score)
      define('industry_average', industry_average)
      define('individual_scores', individual_scores)
      
      # CRITICAL: Verify individual_scores was created successfully
      log(f"üîç FORCE: individual_scores created with {len(individual_scores)} items")
      log(f"üîç FORCE: Sample individual_scores: {individual_scores[:2] if individual_scores else 'EMPTY'}")
      
      # Generate lollipop plot after scores are calculated
      log("üîç FORCE: Generating lollipop plot with calculated scores...")
      need('generate_lollipop_plot_simple')
      
      # Set assessment completion date
      from datetime import datetime
      define('assessment_date', datetime.now().strftime('%B %d, %Y'))
      progress_log.append('Scores ready.')
      
      # CRITICAL: Verify scores are actually calculated before proceeding
      log(f"üîç FORCE: After score calculation - overall_score: {overall_score if defined('overall_score') else 'UNDEFINED'}")
      log(f"üîç FORCE: After score calculation - industry_average: {industry_average if defined('industry_average') else 'UNDEFINED'}")
      log(f"üîç FORCE: After score calculation - category_averages: {category_averages if defined('category_averages') else 'UNDEFINED'}")
      
    except Exception as e:
      log(f"‚ö†Ô∏è Score prep error: {e}")
      progress_log.append(f"Score prep error: {e}")
    
    # =============================================================================
    # AI BACKGROUND TASK LAUNCH
    # =============================================================================
    # Purpose: Launch AI background task with calculated assessment data
    # Input: Calculated scores, pain points, assessment data
    # Output: Background AI task handle, processing state flags
    # Usage: Initiates AI processing for insights generation
    
    # Prepare assessment data for background task - ONLY after scores are calculated
    assessment_data_for_ai = {
      'overall_score': overall_score,
      'industry_average': industry_average,
      'category_averages': category_averages,
      'pain_points': pain_points if defined('pain_points') else []
    }
    
    log(f"üîç FORCE: Passing assessment data to background task: {assessment_data_for_ai}")
    
    # Launch background AI - ALWAYS launch it
    processing_started = True
    processing_complete = False
    log("üöÄ FORCE: About to launch background_action('run_ai_chunks_v2')")
    try:
      ai_task = background_action('run_ai_chunks_v2', assessment_data=assessment_data_for_ai)
      log("‚úÖ FORCE: background_action('run_ai_chunks_v2') launched and handle stored in ai_task")
      background_started = True
    except Exception as ai_launch_error:
      log(f"‚ùå FORCE: Failed to launch background AI task: {ai_launch_error}")
      processing_started = False
    # Processing screen will render next due to show-if on phase
  
  # =============================================================================
  # AI BACKGROUND TASK MONITORING AND RESULT MERGING
  # =============================================================================
  # Purpose: Monitor AI background task and merge results when complete
  # Input: ai_task handle, background processing results
  # Output: Merged AI insights, processing completion flags
  # Usage: Polls background task and integrates AI-generated content
  
  # While still on the processing phase, poll the background AI task and merge results when ready
  try:
    if defined('ai_task') and ai_task.ready():
      _ai_result = ai_task.get()
      if isinstance(_ai_result, dict):
        try:
          # Merge AI-generated insights into session variables
          if 'executive_summary' in _ai_result:
            executive_summary = _ai_result.get('executive_summary')
            define('executive_summary', executive_summary)
            log(f"‚úÖ MERGED: executive_summary ({len(executive_summary)} chars)")
          if 'contradictions_insights' in _ai_result:
            contradictions_insights = _ai_result.get('contradictions_insights')
            define('contradictions_insights', contradictions_insights)
            log(f"‚úÖ MERGED: contradictions_insights ({len(contradictions_insights)} chars)")
          if 'challenge_questions' in _ai_result:
            challenge_questions = _ai_result.get('challenge_questions')
            define('challenge_questions', challenge_questions)
            log(f"‚úÖ MERGED: challenge_questions ({len(challenge_questions)} chars)")
          if 'recommended_services' in _ai_result:
            recommended_services = _ai_result.get('recommended_services')
            define('recommended_services', recommended_services)
            log(f"‚úÖ MERGED: recommended_services ({len(recommended_services)} chars)")
          if 'top_three_offerings' in _ai_result:
            top_three_offerings = _ai_result.get('top_three_offerings')
            define('top_three_offerings', top_three_offerings)
            log(f"‚úÖ MERGED: top_three_offerings ({top_three_offerings})")
          # Merge token usage data
          if 'total_prompt_tokens' in _ai_result:
            total_prompt_tokens = _ai_result.get('total_prompt_tokens')
            define('total_prompt_tokens', total_prompt_tokens)
            log(f"‚úÖ MERGED: total_prompt_tokens ({total_prompt_tokens})")
          if 'total_completion_tokens' in _ai_result:
            total_completion_tokens = _ai_result.get('total_completion_tokens')
            define('total_completion_tokens', total_completion_tokens)
            log(f"‚úÖ MERGED: total_completion_tokens ({total_completion_tokens})")
          if 'total_tokens' in _ai_result:
            total_tokens = _ai_result.get('total_tokens')
            define('total_tokens', total_tokens)
            log(f"‚úÖ MERGED: total_tokens ({total_tokens})")
          # Compute other_available_offerings now that top_three_offerings is known
          try:
            _canonical_offerings = [
              'Assessment Architecture & Build Package',
              'Clarity & Visibility Accelerator',
              'Growth & Loyalty Booster',
              'Lead Flow Builder',
              'Maturity Model Design Sprint',
              'Project Excellence Toolkit',
              'Proposal & Pitch Package',
              'Services That Sell Workshop'
            ]
            _chosen = set(top_three_offerings) if defined('top_three_offerings') and isinstance(top_three_offerings, list) else set()
            other_available_offerings = [n for n in _canonical_offerings if n not in _chosen]
            define('other_available_offerings', other_available_offerings)
            log(f"‚úÖ Computed other_available_offerings: {other_available_offerings}")
          except Exception as _e:
            log(f"‚ö†Ô∏è Failed computing other_available_offerings: {_e}")
          log("‚úÖ SUCCESS: All AI results merged from background task into session")
          # Set AI processing complete flag when all results are successfully merged
          ai_processing_complete = True
          define('ai_processing_complete', True)
          log("‚úÖ AI processing complete flag set to True")
        except Exception as _merge_err:
          log(f"‚ö†Ô∏è Error merging AI results: {_merge_err}")
      else:
        log(f"‚ö†Ô∏è Background task returned non-dict result: {type(_ai_result).__name__}")
      processing_complete = True
      log("üéØ processing_complete set True (background task ready)")
  except Exception as _bg_err:
    log(f"‚ö†Ô∏è Error checking ai_task readiness: {_bg_err}")

  # =============================================================================
  # RESULTS PHASE TRANSITION
  # =============================================================================
  # Purpose: Transition to results phase when processing is complete
  # Input: Processing completion flags, user navigation signals
  # Output: Results phase activation, cleanup of processing state
  # Usage: Final step - advances to results display when ready
  
  # Step 4b: If processing has completed in the background, auto-advance to results
  if (defined('processing_complete') and processing_complete) or (defined('proceeded_to_results') and proceeded_to_results) or (defined('proceeded_to_results') and proceed_to_results):
    try:
      # Set navigation flags and transition to results phase
      proceeded_to_results = True
      proceed_to_results = True
      phase = 'results'
      ready_for_processing = False
      processing_started = False
      if defined('need_category_feedback'):
        del need_category_feedback
      feedback_provided = True
      log('‚úÖ Advancing to results (post-processing or user proceed)')
    except Exception:
      pass
    need('show_assessment_results')
