---
metadata:
  title: Consulting Services Diagnostic
  short title: 
  version: "0.1.1"
  description: |
    Complete diagnostic system for consulting services.
    
    Features:
    - User information collection and validation
    - Interactive drag-and-drop pain points prioritization
    - Dynamic question loading from CSV with 25 assessment questions
    - Real-time scoring and industry benchmarking
    - AI-powered contextual help generation using OpenAI (category-specific)
    - Professional report generation with Word templates
    - Visualizations (radar and lollipop charts)
    - Pain points analysis and service recommendations
    - Document generation with offering-specific content
    
    Architecture:
    - Modular design with separated concerns
    - Centralized configuration management via config.yml
    - Robust error handling and fallbacks
    - Pickle-safe document processing
    - State management for complex navigation flows
    - Background processing for AI content generation
    - Category-aware help system with intelligent regeneration
    
    Dependencies:
    
    # =============================================================================
    # CONFIGURATION & SETUP FILES
    # =============================================================================
    - config.yml: Centralized configuration and constants
    - features.yml: UI/UX configuration and styling
    - variables.yml: Core variable initialization
    
    # =============================================================================
    # DATA PROCESSING & LOADING
    # =============================================================================
    - data_loader.yml: CSV data loading and processing
    - pain_points.yml: Interactive pain points prioritization
    
    # =============================================================================
    # ASSESSMENT CORE LOGIC
    # =============================================================================
    - questions.yml: Assessment questions and scoring with category-aware help
    - scoring.yml: Score calculation algorithms
    - assessment_logic.yml: Main business logic and flow control
    - assessment_intelligence_agent.yml: AI-powered contextual help generation
    
    # =============================================================================
    # VISUALIZATION COMPONENTS
    # =============================================================================
    - radar_chart.yml: Radar chart visualization
    - lollipop_chart.yml: Lollipop chart visualization
    
    # =============================================================================
    # AI & DOCUMENT GENERATION
    # =============================================================================
    - ai_generation.yml: AI content generation
    - document_generator.yml: Document processing and generation
    
    # =============================================================================
    # UI COMPONENTS & DISPLAY
    # =============================================================================
    - title_page.yml: Title page generation
    - processing.yml: Processing status display
     
    Flow States:
    1. welcome_assessment -> User information collection
    2. questions.yml -> Question answering (25 questions)
    3. processing.yml -> AI processing and scoring
    4. show_assessment_results -> Results display and document generation
    
    Error Handling:
    - Graceful fallback for missing logo files
    - Safe defaults for configuration loading
    - Comprehensive logging for debugging
    - Pickle-safe document processing
    - State recovery for interrupted sessions
    
    Author: Veloquent Consulting
    Last Updated: January 2025
    License: Proprietary - Veloquent Consulting
  required privileges:
    - admin
  tags:
    - assessment
    - complete
    - working
    - business-maturity
    - consulting-services
    - ai-powered
    - document-generation
  exit link: off
  logo: |
    <img src="/packagestatic/docassemble.playground1Sep19/logo.png" alt="Logo" class="brand-logo" style="height:50px; vertical-align:middle;">
  
  head: |
    <link rel="icon" type="image/png" href="packagestatic/docassemble.playground1Sep19/logo.png">
    <link rel="shortcut icon" type="image/png" href="packagestatic/docassemble.playground1Sep19/logo.png">
    <link rel="apple-touch-icon" type="image/png" href="packagestatic/docassemble.playground1Sep19/logo.png">
  
  footer: |
    <div class="ve-footer">
      <div>© 2025 Veloquent Consulting    |    diagnose quickly. propose boldly</div>
    </div>

---
include:
  - config.yml  # Configuration constants (NEW)
  - assessment_intelligence_agent.yml  # Phase 1: Assessment Intelligence Agent
  - variables.yml
  - data_loader.yml
  - questions.yml
  - scoring.yml
  - radar_chart.yml
  - lollipop_chart.yml
  - ai_generation.yml  # AI functionality enabled - CHUNKED APPROACH
  - pain_points.yml
  - document_generator.yml  # SIMPLE PICKLE-SAFE VERSION
  - features.yml  # Extracted features configuration
  - title_page.yml  # Extracted title page component
  - assessment_logic.yml  # Extracted main assessment flow logic
  - processing.yml  # Extracted processing screen component

allow:
  - status_json
  - run_ai_chunks
  - go_results
  - send_email
  - test_email

---
# =============================================================================
# CORE DATA OBJECTS
# =============================================================================
# Purpose: Define the primary data structures used throughout the assessment
# These objects store user data, processing results, and generated content
# All objects are designed for pickle safety and session persistence

objects:
  # User information storage with validation
  - user_information: DADict
    # Purpose: Centralized storage for user/organization data
    # Fields: first_name, last_name, company, role, email, industry
    # Validation: Required fields enforced in welcome_assessment
    # Usage: Template variable source for document generation
    
  # Debug information for troubleshooting
  - debug_info: DAList.using(auto_gather=False)
    # Purpose: Collect debug information during processing
    # Usage: Log system state, errors, and processing steps
    # Content: Error messages, processing timestamps, state transitions
    
  # Document generation output
  - merged_assessment_doc: DAFile
    # Purpose: Final generated assessment report
    # Format: Word document with AI insights and visualizations
    # Content: Executive summary, insights, recommendations, charts
    
  # Service offering documents
  - offering_document_files: DADict.using(auto_gather=False, complete_attribute='gathered')
    # Purpose: Store service offering document content
    # Usage: Populate document templates with relevant service information
    # Structure: Key-value pairs of service names and document content
    
  # Processing progress tracking
  - progress_log: DAList.using(auto_gather=False)
    # Purpose: Track processing steps and timing
    # Usage: Monitor AI processing, scoring, and document generation
    # Content: Step descriptions, timestamps, completion status
    
  # Logo file reference
  - logo_file: DAStaticFile.using(filename='logo.png')
    # Purpose: Provide consistent logo access across all screens
    # Usage: Navbar branding, document headers, UI consistency
    # Fallback: Graceful handling if logo file is missing


---
# =============================================================================
# SYSTEM INITIALIZATION
# =============================================================================
# Purpose: Initialize core system components, load configuration,
#          and prepare the assessment environment
# Dependencies: config.yml, features.yml
# Outputs: global_logo_url, system_ready_flag
# Error Handling: Graceful fallback for missing resources

initial: True
id: initialize_system
code: |
  """
  SYSTEM INITIALIZATION BLOCK
  
  This block performs the following critical setup tasks:
  1. Loads all configuration constants from external sources
  2. Initializes core variables using centralized system
  3. Sets up logo handling with fallback mechanisms
  4. Establishes global variables for template processing
  
  Configuration Loading Order:
  - load_configuration_constants: Core assessment settings
  - processing_config: Background task parameters
  - formatting_config: UI and document formatting
  - scoring_config: Assessment scoring algorithms
  - ui_config: User interface settings
  - ai_config: AI processing parameters
  - validate_configuration: Configuration validation
  
  Error Handling:
  - Graceful fallback for missing logo files
  - Safe defaults for configuration loading
  - Comprehensive logging for debugging
  
  Dependencies: config.yml, features.yml
  """
  
  # Initialize configuration system first
  need('load_configuration_constants')
  need('processing_config')
  need('formatting_config')  # Load formatting configuration
  need('scoring_config')
  need('ui_config')
  need('ai_config')
  # Ensure intelligence agent config is loaded so use_intelligence_agent is defined
  try:
    need('intelligence_agent_config')
  except Exception as _iac_err:
    log(f"⚠️ Could not load intelligence_agent_config: {_iac_err}")
  need('validate_configuration')
  
  # Initialize core variables using centralized system
  need('initialize_core_variables')
  
  # Get logo path dynamically using same technique as JSON files
  try:
    from docassemble.base.util import path_and_mimetype
    logo_path, _ = path_and_mimetype('logo.png')
    if logo_path:
      logo_url = logo_path
      log(f"✅ Logo path found: {logo_path}")
    else:
      logo_url = logo_file.url_for()
      log(f"✅ Logo URL from DAStaticFile: {logo_url}")
  except Exception as e:
    logo_url = logo_file.url_for()
    log(f"⚠️ Using fallback logo URL: {logo_url}")
  
  # Debug: logo ready
  
  # Set global logo URL for use in templates
  global_logo_url = logo_url
  
  # Also set a fallback direct URL
  direct_logo_url = '/packagestatic/docassemble.playground1Aug25Optimization/logo.png'
  
  log(f"✅ System initialization complete - global_logo_url: {global_logo_url}")

---
# =============================================================================
# ASSESSMENT FLOW CONTROL
# =============================================================================
# Purpose: Manage complex navigation logic between different assessment phases
#          while preventing infinite loops and ensuring proper state transitions
# Dependencies: assessment_logic.yml
# State Management: Handles early exits, processing completion, and navigation

id: flow_control
mandatory: True
code: |
  """
  FLOW CONTROL: Handle early exits and navigation logic
  
  This block manages the complex state transitions in the assessment flow:
  
  Flow States:
  1. welcome_assessment -> User information collection
      2. questions.yml -> Question answering (25 questions)
      3. processing.yml -> AI processing and scoring
  4. show_assessment_results -> Results display and document generation
  
  State Transitions:
  - Early exit handling for users who proceed directly to results
  - Processing completion detection and auto-advance
  - Category feedback integration
  - Document generation preparation
  
  Error Prevention:
  - Prevents infinite loops in navigation
  - Cleans up stale state flags
  - Ensures proper variable initialization
  """
  
  # FLOW CONTROL: Handle early exits and navigation logic
  if defined('proceeded_to_results') and proceeded_to_results:
    # User has already proceeded to results - skip all processing
    ready_for_processing = False
    processing_started = False
    processing_complete = True
    questions_processed = True
    feedback_provided = True
    
    # Clean up any feedback flags
    if defined('need_category_feedback'):
      del need_category_feedback
    
    # Navigate to results
    need('comprehensive_variables_defined')
    
    # FINAL PAIN POINTS CALCULATION - Right before document generation
    log("🔍 FINAL PAIN POINTS CALCULATION - Right before document generation")
    
    # Ensure pain points CSV data is loaded
    need('load_pain_points_csv')
    
    # Ensure pain points count is properly calculated for template
    if defined('user_selected_challenges') and user_selected_challenges:
      log(f"🔍 FINAL DEBUG: user_selected_challenges type: {type(user_selected_challenges)}")
      log(f"🔍 FINAL DEBUG: user_selected_challenges content: {user_selected_challenges}")
      
      # Check if it's a DADict that needs to be gathered
      if hasattr(user_selected_challenges, 'gathered') and not user_selected_challenges.gathered:
        log("🔍 FINAL DEBUG: DADict not gathered - attempting to gather...")
        try:
          user_selected_challenges.gathered = True
          log("🔍 FINAL DEBUG: DADict gathered successfully")
        except Exception as e:
          log(f"🔍 FINAL DEBUG: Error gathering DADict: {e}")
      
      # Handle both DADict and regular dict
      if hasattr(user_selected_challenges, 'items'):
        log("🔍 FINAL DEBUG: Processing as dict-like object")
        all_items = list(user_selected_challenges.items())
        log(f"🔍 FINAL DEBUG: All items: {all_items}")
        
        selected_challenges = [k for k, v in all_items if v and k != 'none_above']
        log(f"🔍 FINAL DEBUG: Selected items (excluding none_above): {selected_challenges}")
      else:
        log("🔍 FINAL DEBUG: Not a dict-like object, trying alternative processing")
        selected_challenges = []
        try:
          for k, v in user_selected_challenges:
            if v and k != 'none_above':
              selected_challenges.append(k)
          log(f"🔍 FINAL DEBUG: Selected items from iteration: {selected_challenges}")
        except Exception as e:
          log(f"🔍 FINAL DEBUG: Error processing non-dict object: {e}")
          selected_challenges = []
      
      selected_pain_points_count = len(selected_challenges)
      pain_points_selected_count = selected_pain_points_count
      selected_challenges_count = selected_pain_points_count
      log(f"✅ Final pain points count calculation: {selected_pain_points_count}")
      
      # Populate selected_pain_points_details with actual pain point information
      selected_pain_points_details = []
      
      # Debug: Check what's in pain_points_csv_data
      log(f"🔍 FINAL DEBUG: pain_points_csv_data type: {type(pain_points_csv_data) if defined('pain_points_csv_data') else 'undefined'}")
      if defined('pain_points_csv_data') and pain_points_csv_data:
        log(f"🔍 FINAL DEBUG: pain_points_csv_data keys: {list(pain_points_csv_data.keys()) if hasattr(pain_points_csv_data, 'keys') else 'no keys method'}")
        log(f"🔍 FINAL DEBUG: pain_points_csv_data sample: {list(pain_points_csv_data.items())[:2] if hasattr(pain_points_csv_data, 'items') else 'no items method'}")
        
        for challenge_id in selected_challenges:
          log(f"🔍 FINAL DEBUG: Checking challenge_id: {challenge_id}")
          if challenge_id in pain_points_csv_data:
            pain_point = pain_points_csv_data[challenge_id]
            log(f"🔍 FINAL DEBUG: Found pain_point data: {pain_point}")
            selected_pain_points_details.append({
              'title': pain_point.get('title', challenge_id),
              'description': pain_point.get('description', 'No description available')
            })
            log(f"🔍 FINAL DEBUG: Added pain point: {challenge_id} -> {pain_point.get('title', challenge_id)}")
          else:
            log(f"🔍 FINAL DEBUG: challenge_id {challenge_id} NOT found in pain_points_csv_data")
      else:
        log("🔍 FINAL DEBUG: pain_points_csv_data is not defined or empty")
      
      log(f"🔍 FINAL DEBUG: selected_pain_points_details populated with {len(selected_pain_points_details)} items")
      log(f"🔍 FINAL DEBUG: selected_pain_points_details content: {selected_pain_points_details}")
    else:
      log("🔍 FINAL DEBUG: user_selected_challenges is not defined or empty")
      selected_pain_points_count = 0
      pain_points_selected_count = 0
      selected_challenges_count = 0

    need('show_assessment_results')

---
# =============================================================================
# USER INFORMATION COLLECTION
# =============================================================================
# Purpose: Collect and validate user/organization information for personalization
# Validation: Required fields, email format, industry classification
# Outputs: user_information DADict, user_info_collected flag
# Usage: Template variables for document generation and personalization

id: welcome_assessment
mandatory: False
question: |
  **Let's get started with some basic information about you and your organization.**
  
logo: |
  <img src="${ logo_file.url_for() }" alt="Logo" class="brand-logo" style="height:50px; vertical-align:middle;">

subquestion: |
  <div class="assessment-card">
  <p>This information helps us personalize your assessment results and recommendations.</p>
  </div>
  
fields:
  # Required personal information
  - First Name: user_first_name
    required: True
    # Purpose: Personalize assessment results and document generation
    
  - Last Name: user_last_name
    required: True
    # Purpose: Complete name for document headers and personalization
    
  - Company Name: user_company
    required: True
    # Purpose: Organization context for industry benchmarking and document naming
    
  # Role classification for targeted recommendations
  - Your Role: user_role
    choices:
      - Executive/CEO
      - Senior Manager
      - Department Head
      - Team Lead
      - Consultant
      - Other
    required: True
    # Purpose: Tailor insights and recommendations to decision-making level
    
  # Optional contact information
  - Email: user_email
    datatype: email
    required: False
    # Purpose: Follow-up communications and report delivery
    
  # Industry classification for benchmarking
  - Industry: user_industry
    choices:
      - Technology
      - Healthcare
      - Financial Services
      - Professional Services
      - Manufacturing
      - Retail
      - Education
      - Non-Profit
      - Other
    required: False
    # Purpose: Industry-specific benchmarking and recommendations

continue button field: user_info_collected

---
# =============================================================================
# SAFETY CHECKS AND VALIDATION
# =============================================================================
# Purpose: Ensure all required data is available before proceeding to critical operations
# Checks: User information, questions completion, critical variables, AI processing status
# Fallback Mechanisms: Creates default values when data is missing
# Error Recovery: Graceful degradation and comprehensive logging

id: safety_checks
code: |
  """
  SAFETY CHECKS AND VALIDATION
  
  This block ensures all required data is available before proceeding
  to critical operations like document generation and results display.
  
  Checks Performed:
  1. User information completeness and validation
  2. Questions completion status verification
  3. Critical variables availability (scores, averages)
  4. AI processing status and content generation
  
  Fallback Mechanisms:
  - Creates default user_information if missing
  - Logs warnings for missing AI variables
  - Provides safe defaults for critical calculations
  
  Error Recovery:
  - Graceful degradation when data is incomplete
  - Clear logging for troubleshooting
  - User-friendly error messages
  """
  
  # SAFETY CHECKS: Ensure all required data is available
  
  # Check user info collection
  if user_info_collected:
    log(f"SUCCESS: User info collected: {user_first_name} at {user_company}")
  
  # Check questions completion
  if defined('questions_complete') and questions_complete:
    log("SUCCESS: Questions completed")
  else:
    log("INFO: Questions not yet completed")
  
  # Ensure user_information is available
  if not defined('user_information') or not user_information:
    if defined('user_first_name') and defined('user_company'):
      user_information = DADict('user_information', auto_gather=False)
      user_information['first_name'] = user_first_name
      user_information['last_name'] = user_last_name if defined('user_last_name') else ''
      user_information['company'] = user_company
      user_information['role'] = user_role if defined('user_role') else ''
      user_information['email'] = user_email if defined('user_email') else ''
      user_information['industry'] = user_industry if defined('user_industry') else ''
      user_information.gathered = True
      log("SUCCESS: Recreated user_information")
    else:
      # Create fallback user_information
      user_information = DADict('user_information', auto_gather=False)
      user_information['first_name'] = 'User'
      user_information['last_name'] = ''
      user_information['company'] = 'Your Organization'
      user_information['role'] = ''
      user_information['email'] = ''
      user_information['industry'] = ''
      user_information.gathered = True
      log("WARNING: Created fallback user_information")
  else:
    log("SUCCESS: user_information available")
  
  # Verify critical variables
  critical_vars = ['overall_score', 'industry_average', 'category_scores']
  missing_critical = [var for var in critical_vars if not defined(var)]
  
  if missing_critical:
    for var in missing_critical:
      log(f"ERROR: {var} missing for final screen")
  else:
    log("SUCCESS: All critical variables ready")
  
  # Check AI variables
  ai_variables = ['executive_summary', 'contradictions_insights', 'challenge_questions', 'recommended_services', 'top_three_offerings']
  missing_ai_vars = [var for var in ai_variables if not defined(var)]
  
  if missing_ai_vars:
    log(f"WARNING: AI variables undefined: {', '.join(missing_ai_vars)}")
  else:
    log("SUCCESS: All AI variables defined")
  
  log("SUCCESS: All safety checks completed")

---
# =============================================================================
# DATA PREPARATION AND PROCESSING
# =============================================================================
# Purpose: Prepare all data required for assessment results and document generation
# Processing Order: User info -> Plots -> Pain points -> Scores -> AI content -> Templates
# Dependencies: Various included files for specialized processing

id: user_information_template_ready
# ENSURE USER_INFORMATION IS READY FOR TEMPLATE PROCESSING
code: |
  # Use centralized user information initialization
  need('initialize_user_information')

---
# INITIALIZE PLOT VARIABLES
code: |
  # Use centralized plot variables initialization
  need('initialize_plot_variables')

---
# CALCULATE PAIN POINTS DATA
code: |
  # Use centralized pain points calculation
  need('calculate_pain_points_data')
  
  # FINAL PAIN POINTS CALCULATION - This runs right before document generation
  log("🔍 FINAL PAIN POINTS CALCULATION - Right before document generation")
  
  # Ensure pain points count is properly calculated for template
  if defined('user_selected_challenges') and user_selected_challenges:
    # Recalculate to ensure accuracy
    if hasattr(user_selected_challenges, 'items'):
      selected_challenges = [k for k, v in user_selected_challenges.items() if v and k != 'none_above']
    else:
      selected_challenges = []
      for k, v in user_selected_challenges.items():
        if v and k != 'none_above':
          selected_challenges.append(k)
    selected_pain_points_count = len(selected_challenges)
    pain_points_selected_count = selected_pain_points_count
    selected_challenges_count = selected_pain_points_count
    log(f"✅ Final pain points count calculation: {selected_pain_points_count}")
    
    # Debug: Show what was actually selected
    log(f"🔍 FINAL DEBUG: user_selected_challenges type: {type(user_selected_challenges)}")
    if hasattr(user_selected_challenges, 'items'):
      all_items = list(user_selected_challenges.items())
      log(f"🔍 FINAL DEBUG: All items: {all_items}")
      selected_items = [(k, v) for k, v in all_items if v and k != 'none_above']
      log(f"🔍 FINAL DEBUG: Selected items (excluding none_above): {selected_items}")
    else:
      log(f"🔍 FINAL DEBUG: user_selected_challenges content: {user_selected_challenges}")
  else:
    log("🔍 FINAL DEBUG: user_selected_challenges is not defined or empty")
    selected_pain_points_count = 0
    pain_points_selected_count = 0
    selected_challenges_count = 0

---
event: status_json
code: |
  import json
  log_copy = []
  try:
    log_copy = list(progress_log) if defined('progress_log') and progress_log else []
  except Exception:
    log_copy = []
  
  # Check if AI content is ready by looking for the AI variables
  ai_complete = (
    defined('executive_summary') and executive_summary and executive_summary != "Executive summary generation failed" and
    defined('contradictions_insights') and contradictions_insights and contradictions_insights != "Strategic insights generation failed" and
    defined('challenge_questions') and challenge_questions and challenge_questions != "Challenge questions generation failed" and
    defined('recommended_services') and recommended_services and recommended_services != "Recommended services generation failed" and
    defined('top_three_offerings') and top_three_offerings and isinstance(top_three_offerings, list) and len(top_three_offerings) == 3
  )
  
  # Debug: Log the status of each AI variable
  log(f"🔍 STATUS CHECK: executive_summary={defined('executive_summary')}, contradictions_insights={defined('contradictions_insights')}, challenge_questions={defined('challenge_questions')}, recommended_services={defined('recommended_services')}, top_three_offerings={defined('top_three_offerings')}")
  log(f"🔍 AI COMPLETE: {ai_complete}")
  
  complete = (defined('processing_complete') and processing_complete) or ai_complete
  started = (defined('processing_started') and processing_started)
  
  response(json.dumps({
    'log': log_copy, 
    'complete': complete, 
    'started': started,
    'ai_complete': ai_complete
  }), content_type='application/json')

---
event: send_email
code: |
  """
  EMAIL SENDING API ENDPOINT
  
  This endpoint handles email sending requests from the frontend.
  It processes the send_document_email action and returns JSON response.
  """
  
  import json
  
  try:
    # Get the action from the request
    action = request.json.get('action', '') if hasattr(request, 'json') and request.json else ''
    
    if action == 'send_document_email':
      # Trigger the email sending function
      need('send_document_email')
      
      # Return response based on email result
      if defined('email_sent') and email_sent:
        response(json.dumps({
          'success': True,
          'message': 'Email sent successfully'
        }), content_type='application/json')
      else:
        error_msg = email_error if defined('email_error') and email_error else 'Unknown error'
        response(json.dumps({
          'success': False,
          'error': error_msg
        }), content_type='application/json')
    else:
      response(json.dumps({
        'success': False,
        'error': 'Invalid action'
      }), content_type='application/json')
      
  except Exception as e:
    log(f"❌ Email API error: {e}")
    response(json.dumps({
      'success': False,
      'error': f'Server error: {str(e)}'
    }), content_type='application/json')
  
---
# CALCULATE INDIVIDUAL SCORES
code: |
  # Use centralized individual scores calculation
  need('calculate_individual_scores')
  
  # Ensure lollipop plot is generated (fallback if not done in flow logic)
  if not defined('lollipop_plot_generated') or not lollipop_plot_generated:
    need('generate_lollipop_plot_simple')
  
---
# SETUP CATEGORY AVERAGES
code: |
  # Use centralized category averages setup
  need('setup_category_averages')
  
---
# SETUP AI VARIABLES
code: |
  # Use centralized AI variables setup
  need('setup_ai_variables')
  need('setup_count_variables')
  need('setup_assessment_date')

---
# PREPARE TEMPLATE DATA
code: |
  # Use centralized template data preparation
  need('prepare_template_data')
  
---
# LOAD EXTERNAL DATA
code: |
  # Use centralized external data loading and processing
  need('load_external_data')
  need('process_pain_points_details')
  need('cleanup_pickle_safety')

---
# =============================================================================
# DOCUMENT TEMPLATE PREPARATION
# =============================================================================
# Purpose: Prepare all variables required for document template generation
# Content: AI-generated insights, user data, scores, and service offerings
# Validation: Ensures all required variables are defined with fallbacks
# Output: Document-ready variables for Word template processing

id: prepare_document_variables
mandatory: True
code: |
  """
  DOCUMENT TEMPLATE PREPARATION
  
  This block ensures all variables required for document generation are properly
  defined and available for the Word template processing.
  
  Variables Prepared:
  - executive_summary: AI-generated executive summary
  - contradictions_insights: Strategic insights and contradictions
  - challenge_questions: Thought-provoking questions
  - recommended_services: AI-suggested consulting services
  
  Fallback Handling:
  - Provides default messages if AI generation fails
  - Sets ai_content_ready flag for template logic
  - Logs variable status for debugging
  
  Dependencies: ai_generation.yml, document_generator.yml
  """
  
  # Ensure document loader is called
  need('load_offering_document_content')
  
  # Ensure AI variables are defined for document template
  if not defined('executive_summary'):
    executive_summary = "Executive summary generation failed"
  if not defined('contradictions_insights'):
    contradictions_insights = "Strategic insights generation failed"
  if not defined('challenge_questions'):
    challenge_questions = "Challenge questions generation failed"
  if not defined('recommended_services'):
    recommended_services = "Recommended services generation failed"
  
  # Ensure all variables are explicitly defined for the document template
  define('executive_summary', executive_summary)
  define('contradictions_insights', contradictions_insights)
  define('challenge_questions', challenge_questions)
  define('recommended_services', recommended_services)
  
  # CRITICAL: Set a flag to indicate AI processing status
  if executive_summary != "Executive summary generation failed":
    ai_content_ready = True
  else:
    ai_content_ready = False
  
  define('ai_content_ready', ai_content_ready)
  
  log(f"📄 Document template variables set: executive_summary={len(executive_summary)} chars, contradictions_insights={len(contradictions_insights)} chars, ai_content_ready={ai_content_ready}")

  # Build list of other available services as canonical minus top_three_offerings
  try:
    _canonical_offerings = [
      'Assessment Architecture & Build Package',
      'Clarity & Visibility Accelerator',
      'Growth & Loyalty Booster',
      'Lead Flow Builder',
      'Maturity Model Design Sprint',
      'Project Excellence Toolkit',
      'Proposal & Pitch Package',
      'Services That Sell Workshop'
    ]
    _chosen = set(top_three_offerings) if defined('top_three_offerings') and isinstance(top_three_offerings, list) else set()
    other_available_offerings = [name for name in _canonical_offerings if name not in _chosen][:5]
    define('other_available_offerings', other_available_offerings)
    log(f"📄 other_available_offerings set to: {other_available_offerings}")
    
    # TESTING OUTPUT: Display top 3 and other services for verification
    log("=" * 60)
    log("🎯 TESTING: TOP 3 RECOMMENDED SERVICES")
    log("=" * 60)
    if defined('top_three_offerings') and top_three_offerings:
      for i, offering in enumerate(top_three_offerings, 1):
        log(f"  {i}. {offering}")
    else:
      log("  ❌ No top 3 offerings available")
    
    log("")
    log("📋 TESTING: OTHER AVAILABLE SERVICES")
    log("=" * 60)
    if other_available_offerings:
      for i, offering in enumerate(other_available_offerings, 1):
        log(f"  {i}. {offering}")
    else:
      log("  ❌ No other services available")
    log("=" * 60)
    
    # Additional debugging for template variables
    log("🔍 TEMPLATE VARIABLES DEBUG:")
    log(f"  - top_three_offerings defined: {defined('top_three_offerings')}")
    log(f"  - offering_document_files_list defined: {defined('offering_document_files_list')}")
    if defined('offering_document_files_list'):
      log(f"  - offering_document_files_list length: {len(offering_document_files_list)}")
      for i, doc in enumerate(offering_document_files_list[:3]):  # Show first 3
        log(f"    {i+1}. {doc.name} -> {doc.template_path}")

    # Reorder offering_document_files_list so first three match top_three_offerings strictly
    try:
      if defined('offering_document_files_list') and offering_document_files_list and defined('top_three_offerings') and isinstance(top_three_offerings, list):
        def _norm(s):
          try:
            return (s or '').lower().replace('&','and').replace('—','-').replace('–','-').replace('  ',' ').strip()
          except Exception:
            return ''
        name_to_doc = {}
        for _doc in offering_document_files_list:
          _doc_name = _doc.name if hasattr(_doc, 'name') else None
          _doc_filename = _doc.filename if hasattr(_doc, 'filename') else None
          _doc_tpl = _doc.template_path if hasattr(_doc, 'template_path') else None
          _key = _norm(_doc_name or _doc_filename or _doc_tpl)
          if _key and _key not in name_to_doc:
            name_to_doc[_key] = _doc
        selected_docs = []
        selected_paths = set()
        for off in top_three_offerings:
          _k = _norm(off)
          _d = name_to_doc.get(_k)
          if _d and getattr(_d, 'template_path', None) not in selected_paths:
            selected_docs.append(_d)
            selected_paths.add(_d.template_path)
        others = [_d for _d in offering_document_files_list if getattr(_d, 'template_path', None) not in selected_paths]
        offering_document_files_list = selected_docs + others
        define('offering_document_files_list', offering_document_files_list)
        log("✅ Reordered offering_document_files_list to match top_three_offerings order")
        for i, _d in enumerate(offering_document_files_list[:5]):
          log(f"    ORDER {i+1}: {_d.name} -> {_d.template_path}")
        # Note: top_three_template_paths will be built after AI results are merged
    except Exception as _order_err:
      log(f"⚠️ Failed to reorder offering_document_files_list: {_order_err}")
    
  except Exception as _e:
    log(f"⚠️ Failed to set other_available_offerings: {_e}")
  
  # Ensure top_three_template_paths is always defined for template usage
  if not defined('top_three_template_paths'):
    top_three_template_paths = []
    define('top_three_template_paths', top_three_template_paths)
    log("✅ Fallback: top_three_template_paths set to empty list")

  # ===========================================================================
  # PRECOMPUTE ANSWER LEVELS AND NEXT BEST ACTIONS FOR DOCX TEMPLATE
  # ===========================================================================
  # Purpose: Preprocess assessment data to provide clean, structured variables
  # for the Word document Jinja2 template, eliminating complex parsing logic
  # in the DOCX template itself.
  #
  # Key Changes (v0.0.7):
  # - Precomputes answer_levels: numeric level (0-4) for each user response
  # - Generates next_best_actions_single: single improvement text per question
  # - Creates improvement_matrix: full 5-item improvement arrays for template access
  # - Extracts question_prompts: clean question text for template display
  # - Supports configurable level_prefix_style (letter/number/none) from config
  # - Handles both dict-like and object-like question structures
  # - Provides fallback to CSV data when question fields are missing
  #
  # Template Usage: The DOCX template now uses these precomputed variables
  # instead of complex Jinja2 logic, ensuring reliable improvement text display.
  try:
    # Ensure dependencies are available
    need('formatting_config')

    # Use the globally selected style if available; fall back to config
    parsing_style = None
    try:
      parsing_style = global_level_style if defined('global_level_style') else level_prefix_style
    except Exception:
      parsing_style = level_prefix_style if defined('level_prefix_style') else 'letter'

    # Core template variables for DOCX rendering
    answer_levels = []                    # Numeric level (0-4) for each user response
    next_best_actions = []               # Legacy: level-specific improvement arrays
    next_best_actions_single = []        # Primary: single improvement text per question
    improvement_matrix = []              # Full 5-item improvement arrays for template access
    question_prompts = []                # Clean question text extracted for template display

    # Fallback source from CSV (guaranteed to exist in package)
    # Used when question objects lack proper levels/improvements fields
    csv_improvements_by_index = []
    csv_prompts_by_index = []
    try:
      import csv as _csv
      from docassemble.base.util import path_and_mimetype as _pam
      _fp, _ = _pam('FinalQuestionsIDwScore.csv')
      with open(_fp, encoding='utf-8') as _f:
        _reader = list(_csv.DictReader(_f))
        for _row in _reader:
          # Build improvements list of length 5 with fallbacks
          _imps = []
          for _k in range(5):
            _ival = (_row.get(f'Improvement{_k}', '') or '').strip()
            _imps.append(_ival if _ival else 'Continue current practices')
          csv_improvements_by_index.append(_imps)
          csv_prompts_by_index.append((_row.get('Prompt', '') or '').strip())
    except Exception as _csv_err:
      log(f"⚠️ CSV fallback unavailable: {_csv_err}")

    def _get_field(obj, key, default=None):
      """Safely get a field from dict-like or object-like containers."""
      try:
        if isinstance(obj, dict):
          return obj.get(key, default)
      except Exception:
        pass
      try:
        val = getattr(obj, key)
        return val if val is not None else default
      except Exception:
        pass
      try:
        return obj[key]
      except Exception:
        return default

    # Helper to parse a single answer into level index (0-4), None if unknown
    def _parse_level_from_answer(answer_str, levels, style):
      if not answer_str or not levels:
        return None
      s = str(answer_str).strip()
      # Letter style: "A: ..."
      if style == 'letter' and len(s) >= 2 and s[1] == ':':
        first = s[0].upper()
        if defined('level_mapping') and first in level_mapping:
          idx = level_mapping.get(first, None)
          if idx is not None and 0 <= idx < len(levels):
            return idx
      # Number style: "1: ..."
      if style == 'number' and len(s) >= 2 and s[1] == ':' and s[0].isdigit():
        num = int(s[0])
        idx = num - 1
        if 0 <= idx < len(levels):
          return idx
      # None style: description only
      if style == 'none':
        for i, lvl in enumerate(levels):
          if lvl and str(lvl).strip().lower() == s.lower():
            return i
        for i, lvl in enumerate(levels):
          if lvl and str(lvl).lower() in s.lower():
            return i
      # Fallback: try matching description after ':' for any style
      parts = s.split(':', 1)
      desc = parts[1].strip().lower() if len(parts) > 1 else s.lower()
      for i, lvl in enumerate(levels):
        if lvl and str(lvl).strip().lower() == desc:
          return i
      return None

    # Build arrays aligned with questions
    if defined('questions') and questions and defined('answers') and answers:
      q_len = len(questions)
      a_len = len(answers)
      for i in range(q_len):
        q = questions[i]
        ans = answers[i] if i < a_len else None
        lvls = _get_field(q, 'levels', []) or []
        impr = _get_field(q, 'improvements', []) or []

        # Build a synthesized 5-item improvements list from either fields or array
        synth_impr = []
        try:
          for _k in range(5):
            # Prefer explicit Improvement{n} fields if present
            from_field = _get_field(q, f'Improvement{_k}', None)
            from_array = None
            try:
              if impr and len(impr) > _k:
                from_array = impr[_k]
            except Exception:
              from_array = None
            chosen = (from_field if (isinstance(from_field, str) and from_field.strip()) else from_array)
            chosen = (chosen or '').strip()
            if not chosen:
              chosen = 'Continue current practices'
            synth_impr.append(chosen)
        except Exception:
          # Last resort: ensure a 5-length list of fallbacks
          synth_impr = ['Continue current practices'] * 5

        # If synth_impr still looks empty/fallback-only and CSV has data for this index, use CSV
        try:
          if i < len(csv_improvements_by_index):
            # Replace any empty entries with CSV entries
            _csv_imps = csv_improvements_by_index[i]
            _final_imps = []
            for _k in range(5):
              _local = (synth_impr[_k] if len(synth_impr) > _k else '')
              if not _local or _local == 'Continue current practices':
                _local = _csv_imps[_k]
              _final_imps.append(_local if _local else 'Continue current practices')
            synth_impr = _final_imps
        except Exception:
          pass

        # If levels are missing, also rebuild from Maturity0..4
        if not lvls or len(lvls) < 5:
          try:
            lvls = []
            for _k in range(5):
              mval = _get_field(q, f'Maturity{_k}', '')
              lvls.append((mval or '').strip())
          except Exception:
            lvls = lvls or []

        lvl_idx = _parse_level_from_answer(ans, lvls, parsing_style)
        # Default unknown level to 0 for action lookup, but track None for status
        level_for_action = lvl_idx if (isinstance(lvl_idx, int) and 0 <= lvl_idx <= 4) else 0

        # Determine next best action text
        if isinstance(lvl_idx, int) and lvl_idx == 4:
          nba_text = "Status: You've reached the highest maturity level in this area. Focus on maintaining excellence."
        else:
          candidate = None
          # Use synthesized improvements (guaranteed non-empty entries)
          if synth_impr and 0 <= level_for_action < len(synth_impr):
            candidate = synth_impr[level_for_action]
          elif impr and 0 <= level_for_action < len(impr):
            try:
              candidate = impr[level_for_action]
            except Exception:
              candidate = None
          # Normalize and fallback when empty/missing
          if isinstance(candidate, str):
            candidate = candidate.strip()
          nba_text = candidate if candidate else 'Continue current practices'

        answer_levels.append(lvl_idx if isinstance(lvl_idx, int) else 0)
        next_best_actions.append(nba_text)

        # SINGLE next best action per question (primary template variable)
        # Maps user's selected level to the corresponding improvement text
        # This is the main variable used by the DOCX template for recommendations
        if isinstance(lvl_idx, int) and lvl_idx == 4:
          nba_single = "Status: You've reached the highest maturity level in this area. Focus on maintaining excellence."
        else:
          # Check for single improvement field first (preferred)
          single_field = None
          for key in ('NextBestAction', 'Next_Best_Action', 'Improvement', 'improvement'):
            val = _get_field(q, key, None)
            if isinstance(val, str) and val.strip():
              single_field = val.strip()
              break
          if single_field:
            nba_single = single_field
          else:
            nba_single = nba_text  # fallback to level-based improvement

        next_best_actions_single.append(nba_single)

        # Store synthesized improvements row for template indexing
        # Provides full 5-item array for advanced template logic if needed
        improvement_matrix.append(synth_impr)

        # Extract clean question prompt for template display
        # Ensures the DOCX template gets readable question text without object access
        _prompt = _get_field(q, 'prompt', None)
        if not (isinstance(_prompt, str) and _prompt.strip()):
          if i < len(csv_prompts_by_index):
            _prompt = csv_prompts_by_index[i]
        question_prompts.append((_prompt or '').strip())
    else:
      # Ensure variables exist even if unanswered
      answer_levels = []
      next_best_actions = []

    # Expose precomputed variables to DOCX template
    # These variables eliminate the need for complex Jinja2 logic in the Word document
    define('answer_levels', answer_levels)                    # Numeric levels for each response
    define('next_best_actions', next_best_actions)           # Legacy: level-specific arrays
    define('next_best_actions_single', next_best_actions_single)  # Primary: single improvement per question
    define('question_prompts', question_prompts)             # Clean question text for display
    define('improvement_matrix', improvement_matrix)         # Full improvement arrays for advanced logic
    log(f"📄 Prepared answer_levels ({len(answer_levels)}), next_best_actions ({len(next_best_actions)}), next_best_actions_single ({len(next_best_actions_single)}), improvement_matrix ({len(improvement_matrix)}), question_prompts ({len(question_prompts)})")
  except Exception as _prep_err:
    log(f"⚠️ Failed to precompute next_best_actions: {_prep_err}")

  # Resolve help tokens only; do not generate help on results
  try:
    need('capture_help_tokens_block')
  except Exception as _tok_err:
    log(f"⚠️ Could not run capture_help_tokens: {_tok_err}")

---
# =============================================================================
# TOKEN TRACKING DEPENDENCY
# =============================================================================
# Purpose: Ensure help function token variables are captured in main session
# Issue: Help function runs in separate context via need() - tokens not transferred
# Solution: Force dependency on help token variables to transfer to main session

id: ensure_help_tokens_block
code: |
  # Disabled: Do NOT generate help on results or post-questions phases
  pass

---
id: capture_help_tokens_block
code: |
  # HELP TOKENS
  # Per-call: help_prompt_tokens, help_completion_tokens, help_total_tokens
  # Totals: help_prompt_tokens_total, help_completion_tokens_total, help_total_tokens_total
  # Display prefers totals; triggers one help run if still zero; approximates as last resort.
  # Extract both totals and per-call values without overwriting
  # Read totals (no _internal fallback needed)
  tot_prompt = help_prompt_tokens_total if defined('help_prompt_tokens_total') else 0
  tot_completion = help_completion_tokens_total if defined('help_completion_tokens_total') else 0
  tot_total = help_total_tokens_total if defined('help_total_tokens_total') else 0

  # Read per-call values from globals before we assign anything locally
  try:
    per_prompt = globals().get('help_prompt_tokens', 0)
  except Exception:
    per_prompt = 0
  try:
    per_completion = globals().get('help_completion_tokens', 0)
  except Exception:
    per_completion = 0
  try:
    per_total = globals().get('help_total_tokens', 0)
  except Exception:
    per_total = 0

  # Resolve display values for results/docx:
  # - help_*_tokens: last-call values (accurate for most recent help)
  # - help_*_tokens_total: accumulated totals across allowed calls (per-category)
  help_prompt_tokens = per_prompt or 0
  help_completion_tokens = per_completion or 0
  help_total_tokens = per_total or 0

  # If totals are zero but per-call has data, persist into totals for downstream consumers
  if (tot_prompt or 0) == 0 and (per_prompt or 0) > 0:
    help_prompt_tokens_total = per_prompt
    define('help_prompt_tokens_total', help_prompt_tokens_total)
  if (tot_completion or 0) == 0 and (per_completion or 0) > 0:
    help_completion_tokens_total = per_completion
    define('help_completion_tokens_total', help_completion_tokens_total)
  if (tot_total or 0) == 0 and (per_total or 0) > 0:
    help_total_tokens_total = per_total
    define('help_total_tokens_total', help_total_tokens_total)

  # If still zero/undefined, force a one-shot help generation and re-read
  try:
    if (not help_prompt_tokens) and (not help_completion_tokens) and (not help_total_tokens):
      need('generate_intelligence_agent_content')
      # Re-read totals after generation
      if defined('help_prompt_tokens_total'):
        help_prompt_tokens = help_prompt_tokens_total
      if defined('help_completion_tokens_total'):
        help_completion_tokens = help_completion_tokens_total
      if defined('help_total_tokens_total'):
        help_total_tokens = help_total_tokens_total
  except Exception as _force_err:
    log(f"⚠️ Force help gen failed: {_force_err}")

  # Expose simple variables for UI/doc usage
  define('help_prompt_tokens', help_prompt_tokens)
  define('help_completion_tokens', help_completion_tokens)
  define('help_total_tokens', help_total_tokens)
  # If last-call not provided (older sessions), mirror to last_call vars so DOCX can use either
  if not defined('help_prompt_tokens_last_call'):
    define('help_prompt_tokens_last_call', help_prompt_tokens)
  if not defined('help_completion_tokens_last_call'):
    define('help_completion_tokens_last_call', help_completion_tokens)
  if not defined('help_total_tokens_last_call'):
    define('help_total_tokens_last_call', help_total_tokens)
  define('help_prompt_tokens_total', help_prompt_tokens if not defined('help_prompt_tokens_total') else help_prompt_tokens_total)
  define('help_completion_tokens_total', help_completion_tokens if not defined('help_completion_tokens_total') else help_completion_tokens_total)
  define('help_total_tokens_total', help_total_tokens if not defined('help_total_tokens_total') else help_total_tokens_total)

  # No approximation: if no help ran, values remain 0 to reflect true usage

  # Stable display variables used by the results screen
  help_display_prompt = help_prompt_tokens
  help_display_completion = help_completion_tokens
  help_display_total = help_total_tokens
  define('help_display_prompt', help_display_prompt)
  define('help_display_completion', help_display_completion)
  define('help_display_total', help_display_total)

  log(f"✅ Help tokens ready: prompt={help_prompt_tokens}, completion={help_completion_tokens}, total={help_total_tokens}")
  capture_help_tokens = True
  # Mark this block as satisfied for screens that list it under need:
  capture_help_tokens_block = True

  # Debug strings for DOCX/UI; recompute from latest resolved values with spacing
  try:
    help_tokens_debug = f"{help_prompt_tokens} | {help_completion_tokens} | {help_total_tokens}"
  except Exception:
    help_tokens_debug = "0 | 0 | 0"
  try:
    _mp = total_prompt_tokens if defined('total_prompt_tokens') else 0
    _mc = total_completion_tokens if defined('total_completion_tokens') else 0
    _mt = total_tokens if defined('total_tokens') else 0
    main_tokens_debug = f"{_mp} | {_mc} | { _mt }"
  except Exception:
    main_tokens_debug = "0 | 0 | 0"
  define('help_tokens_debug', help_tokens_debug)
  define('main_tokens_debug', main_tokens_debug)

---
# Ensure help tokens exist for DOCX before attachment render
id: prepare_help_tokens_for_doc
code: |
  try:
    if defined('use_intelligence_agent') and use_intelligence_agent:
      need('generate_intelligence_agent_content')
  except Exception as _e:
    log(f"⚠️ prepare_help_tokens_for_doc failed to generate help: {_e}")
  # Guarantee totals exist
  if not defined('help_prompt_tokens_total'):
    help_prompt_tokens_total = 0
  if not defined('help_completion_tokens_total'):
    help_completion_tokens_total = 0
  if not defined('help_total_tokens_total'):
    help_total_tokens_total = 0
  define('help_prompt_tokens_total', help_prompt_tokens_total)
  define('help_completion_tokens_total', help_completion_tokens_total)
  define('help_total_tokens_total', help_total_tokens_total)
  # Mark this block as satisfied for screens that list it under need:
  prepare_help_tokens_for_doc = True

---
# Transfer main report token variables from background task to main session
id: transfer_main_report_tokens
code: |
  # Ensure main report token variables are available in main session
  # These come from the background task but need explicit transfer
  if not defined('total_prompt_tokens'):
    total_prompt_tokens = 0
  if not defined('total_completion_tokens'):
    total_completion_tokens = 0
  if not defined('total_tokens'):
    total_tokens = 0
  
  # Log the values for debugging
  log(f"📊 Main report tokens: prompt={total_prompt_tokens}, completion={total_completion_tokens}, total={total_tokens}")
  
  # Ensure they're defined in the session
  define('total_prompt_tokens', total_prompt_tokens)
  define('total_completion_tokens', total_completion_tokens)
  define('total_tokens', total_tokens)
  
  # Mark this dependency as satisfied so need: can resolve it
  transfer_main_report_tokens = True

---
# =============================================================================
# ASSESSMENT RESULTS DISPLAY
# =============================================================================
# Purpose: Display final assessment results with AI-generated insights
# Content: Executive summary, strategic insights, challenge questions, recommendations
# Features: Conditional display based on AI processing success
# UI: Professional layout with logo and document download

id: show_assessment_results
mandatory: True
question: |
  ### Your Diagnostic Report is complete!

logo: |
  <img src="${ logo_file.url_for() }" alt="Logo" class="brand-logo" style="height:50px; vertical-align:middle;">

subquestion: |
  Your personalized diagnostic report has been generated and is ready for download below.

attachments:
  # Professional assessment report with AI insights and visualizations
  - name: Consulting Services Business Maturity Diagnostic Report
    filename: Consulting_Services_Business_Maturity_Diagnostic_${user_company.replace(' ', '_') if defined('user_company') and user_company else 'Assessment_Report'}
    docx template file: maturity_assessment_report.docx
    # Template Variables Documentation (Updated v0.0.7):
    # AI-Generated Content:
    #   executive_summary: AI-generated executive summary of assessment results
    #   contradictions_insights: Strategic insights highlighting contradictions in responses
    #   challenge_questions: Thought-provoking questions for further consideration
    #   recommended_services: AI-suggested consulting services based on assessment
    # User Information:
    #   user_first_name, user_last_name: Personal information for document headers
    #   user_company: Organization name for personalization and file naming
    #   user_role: Role classification for targeted recommendations
    #   user_industry: Industry context for benchmarking
    # Precomputed Assessment Data (NEW in v0.0.7):
    #   answer_levels: Numeric level (0-4) for each user response, used for improvement mapping
    #   next_best_actions_single: Single improvement text per question (primary template variable)
    #   question_prompts: Clean question text extracted for template display
    #   improvement_matrix: Full 5-item improvement arrays for advanced template logic
    #   next_best_actions: Legacy level-specific improvement arrays (maintained for compatibility)
    # Assessment Results:
    #   overall_score: Calculated maturity score (0-5 scale)
    #   industry_average: Benchmark score for comparison
    #   category_scores: Detailed scores by assessment category
    #   assessment_date: Date of assessment completion
    # Service Offerings:
    #   offering_document_files_list: List of relevant service documents
    #   merged_offering_content: Combined content from service offerings
    template_variables:
      executive_summary: ${executive_summary if defined('executive_summary') else 'Executive summary generation failed'}
      contradictions_insights: ${contradictions_insights if defined('contradictions_insights') else 'Strategic insights generation failed'}
      challenge_questions: ${challenge_questions if defined('challenge_questions') else 'Challenge questions generation failed'}
      recommended_services: ${recommended_services if defined('recommended_services') else 'Recommended services generation failed'}
      user_first_name: ${user_information.get('first_name', '') if defined('user_information') and user_information else ''}
      user_last_name: ${user_information.get('last_name', '') if defined('user_information') and user_information else ''}
      user_company: ${user_information.get('company', '') if defined('user_information') and user_information else ''}
      user_role: ${user_information.get('role', '') if defined('user_information') and user_information else ''}
      user_industry: ${user_information.get('industry', '') if defined('user_information') and user_information else ''}
      overall_score: ${overall_score if defined('overall_score') else 'N/A'}
      industry_average: ${industry_average if defined('industry_average') else 'N/A'}
      category_scores: ${category_scores if defined('category_scores') else {}}
      assessment_date: ${assessment_date if defined('assessment_date') else 'N/A'}
      offering_document_files_list: ${offering_document_files_list if defined('offering_document_files_list') else []}
      top_three_offerings: ${top_three_offerings if defined('top_three_offerings') else []}
      merged_offering_content: ${merged_offering_content if defined('merged_offering_content') else 'No offering documents available'}
      # Questions Data
      questions: ${questions if defined('questions') else []}
      individual_scores: ${individual_scores if defined('individual_scores') else []}
      answers: ${answers if defined('answers') else []}
      answer_levels: ${answer_levels if defined('answer_levels') else []}
      next_best_actions: ${next_best_actions if defined('next_best_actions') else []}
      next_best_actions_single: ${next_best_actions_single if defined('next_best_actions_single') else []}
      question_prompts: ${question_prompts if defined('question_prompts') else []}
      improvement_matrix: ${improvement_matrix if defined('improvement_matrix') else []}
      global_level_style: ${global_level_style if defined('global_level_style') else 'letter'}
      letter_choices: ${letter_choices if defined('letter_choices') else []}
      level_mapping: ${level_mapping if defined('level_mapping') else {}}
      # Pain Points Data
      selected_pain_points_count: ${selected_pain_points_count if defined('selected_pain_points_count') else 0}
      pain_points_selected_count: ${pain_points_selected_count if defined('pain_points_selected_count') else 0}
      selected_pain_points_details: ${selected_pain_points_details if defined('selected_pain_points_details') else []}
      custom_challenges_text: ${custom_challenges_text if defined('custom_challenges_text') else ''}
      user_selected_challenges: ${user_selected_challenges if defined('user_selected_challenges') else {}}
      # Token usage tracking
      total_prompt_tokens: ${total_prompt_tokens if defined('total_prompt_tokens') else 0}
      total_completion_tokens: ${total_completion_tokens if defined('total_completion_tokens') else 0}
      total_tokens: ${total_tokens if defined('total_tokens') else 0}
      # Help function token usage tracking (use non-total variables to avoid timing issues)
      help_prompt_tokens: ${help_prompt_tokens if defined('help_prompt_tokens') else (help_prompt_tokens_total if defined('help_prompt_tokens_total') else 0)}
      help_completion_tokens: ${help_completion_tokens if defined('help_completion_tokens') else (help_completion_tokens_total if defined('help_completion_tokens_total') else 0)}
      help_total_tokens: ${help_total_tokens if defined('help_total_tokens') else (help_total_tokens_total if defined('help_total_tokens_total') else 0)}
      # Also expose *_total keys for DOCX templates that reference totals directly
      help_prompt_tokens_total: ${help_prompt_tokens_total if defined('help_prompt_tokens_total') else (help_prompt_tokens if defined('help_prompt_tokens') else 0)}
      help_completion_tokens_total: ${help_completion_tokens_total if defined('help_completion_tokens_total') else (help_completion_tokens if defined('help_completion_tokens') else 0)}
      help_total_tokens_total: ${help_total_tokens_total if defined('help_total_tokens_total') else (help_total_tokens if defined('help_total_tokens') else 0)}
      # Debug strings for DOCX to verify values render
      help_tokens_debug: ${ (str(help_prompt_tokens_total) if defined('help_prompt_tokens_total') else '0') + ' | ' + (str(help_completion_tokens_total) if defined('help_completion_tokens_total') else '0') + ' | ' + (str(help_total_tokens_total) if defined('help_total_tokens_total') else '0') }
      main_tokens_debug: ${ (str(total_prompt_tokens) if defined('total_prompt_tokens') else '0') + ' | ' + (str(total_completion_tokens) if defined('total_completion_tokens') else '0') + ' | ' + (str(total_tokens) if defined('total_tokens') else '0') }
    valid formats:
      - docx
      - pdf

# Provide native buttons (hidden via CSS/JS) so footer can proxy-click them
# Do not render native buttons; stylized footer will handle navigation

# =============================================================================
# RESULTS SCREEN UI ENHANCEMENTS
# =============================================================================
# Purpose: Custom UI for results screen with sticky footer and enhanced styling
# Features: Sticky bottom navigation, custom button handling, attachment styling
# JavaScript: Handles button actions, attachment alerts, and UI enhancements

under: |
  <div class="results-footer results-sticky-bar" style="position: sticky; bottom: 0; background: #fff; padding: 12px 0; border-top: 1px solid #eee; display: flex; justify-content: center; z-index: 1000;">
    <div class="d-flex justify-content-center gap-2">
      <button type="button" class="btn btn-outline-secondary" data-action="restart">Restart</button>
      <button type="button" class="btn btn-secondary" data-action="exit">Exit</button>
    </div>
  </div>
  <script>
  (function(){
    """
    RESULTS SCREEN JAVASCRIPT ENHANCEMENTS
    
    This script provides enhanced UI functionality for the results screen:
    
    Features:
    - Body class management for CSS targeting
    - Sticky footer visibility enforcement
    - Native button hiding and custom button handling
    - Attachment alert styling and gap elimination
    - Attachment title italicization
    
    Error Handling:
    - Graceful fallback for missing elements
    - Mutation observers for dynamic content
    - Comprehensive logging for debugging
    """
    
    // Ensure the body has the results class so CSS hides top native buttons
    document.body.classList.add('results-screen');
    
    // Debug: Ensure footer is visible
    console.log('Results screen loaded - checking footer visibility');
    var footer = document.querySelector('.results-footer');
    if (footer) {
      console.log('Footer found:', footer);
      footer.style.setProperty('display', 'flex', 'important');
      footer.style.setProperty('position', 'sticky', 'important');
      footer.style.setProperty('bottom', '0', 'important');
      footer.style.setProperty('z-index', '1000', 'important');
    } else {
      console.log('Footer not found!');
    }
    function submitAction(actionName){
      var form = document.getElementById('daform') || document.querySelector('form.daformmultiplechoice');
      if (!form) return;
      try {
        // Create a hidden input to signal restart/exit and submit the native form
        var inp = document.createElement('input');
        inp.type = 'hidden';
        inp.name = actionName; // 'restart' or 'exit'
        inp.value = '1';
        form.appendChild(inp);
        form.submit();
      } catch(e) {
        console.log('Submit action error:', e);
      }
    }
    function clickRestart(){ submitAction('restart'); }
    function clickExit(){ submitAction('exit'); }
    document.addEventListener('click', function(e){
      var t = e.target;
      if (!t) return;
      if (t.matches('.results-sticky-bar [data-action="restart"]')){ e.preventDefault(); clickRestart(); }
      if (t.matches('.results-sticky-bar [data-action="exit"]')){ e.preventDefault(); clickExit(); }
    });

    // Hide top native buttons on this screen with a simple persistent hide
    function hideTopButtons(){
      // Hide any native top button sets if present
      var form = document.getElementById('daform') || document.querySelector('form.daformmultiplechoice');
      if (!form) return;
      var sets = form.querySelectorAll('fieldset.da-button-set, fieldset.da-field-buttons');
      sets.forEach(function(fs){
        try {
          fs.style.setProperty('display', 'none', 'important');
          fs.style.setProperty('visibility', 'hidden', 'important');
        } catch(e){}
      });
    }

    // Remove any yellow/red buttons (warning/danger) to keep only grey styles
    function hideUnwantedButtons(){
      var unwanted = document.querySelectorAll('button.btn-warning, button.btn-danger, button.btn-outline-warning, button.btn-outline-danger');
      unwanted.forEach(function(btn){
        try { btn.style.setProperty('display', 'none', 'important'); } catch(e){}
      });
    }
    hideTopButtons();
    var mo = new MutationObserver(function(){ hideTopButtons(); hideUnwantedButtons(); });
    mo.observe(document.getElementById('daform') || document.body, { childList: true, subtree: true });
    
    // Also hide buttons immediately and on any DOM changes
    setTimeout(function(){ hideTopButtons(); hideUnwantedButtons(); }, 100);
    setTimeout(function(){ hideTopButtons(); hideUnwantedButtons(); }, 500);
    setTimeout(function){ hideTopButtons(); hideUnwantedButtons(); }, 1000);

    // Force the attachment success alert (green) to render as primary/blue
    // and collapse any gap above it (including stray <hr>, hidden headings, or sibling margins)
    function restyleAttachmentAlert(){
      try {
        // Hide screen-reader-only hidden headings that can add layout gap
        var hiddenH2s = document.querySelectorAll('#daquestion h2.visually-hidden, #daquestion h2.sr-only, #daquestion h2[style*="1px"]');
        hiddenH2s.forEach(function(h){
          h.style.setProperty('display', 'none', 'important');
          h.style.setProperty('margin', '0', 'important');
          h.style.setProperty('padding', '0', 'important');
          if (h.parentNode) { try { h.parentNode.removeChild(h); } catch(e) {} }
        });

        // Remove any <br> directly after the form that creates vertical gap
        var brAfterForm = document.querySelector('#daform + br');
        if (brAfterForm && brAfterForm.parentNode) {
          try { brAfterForm.parentNode.removeChild(brAfterForm); } catch(e) {}
        }

        var alerts = document.querySelectorAll('.da-attachment-alert, .da-attachment-alert-single, #daquestion .alert');
        alerts.forEach(function(el){
          // Eliminate only the white band ABOVE the alert; keep alert size
          el.style.setProperty('margin-top', '0', 'important');
          // Do NOT shrink padding; keep comfortable height
          // Collapse multiple preceding siblings if they introduce space
          var cursor = el.previousElementSibling;
          var safety = 0;
          while (cursor && safety < 5) { // limit walkback
            cursor.style.setProperty('margin-bottom', '0', 'important');
            cursor.style.setProperty('padding-bottom', '0', 'important');
            if (cursor.tagName === 'HR') {
              cursor.style.setProperty('display', 'none', 'important');
              try { cursor.parentNode && cursor.parentNode.removeChild(cursor); } catch(e) {}
            }
            cursor = cursor.previousElementSibling;
            safety++;
          }
        });
      } catch(e) {}
    }

    // Ensure the attachment title renders in italics regardless of structure
    function italicizeAttachmentTitle(){
      try {
        var targets = document.querySelectorAll('#daform .da-attachment-title, #daform .da-attachment-title a, #daform .da-attachment-title-wrapper, #daform .da-attachment-title-wrapper *,'+
                                               ' #daquestion .da-attachment-title, #daquestion .da-attachment-title *,'+
                                               ' .da-attachment-alert + h3, .da-attachment-alert-single + h3');
        targets.forEach(function(node){
          node.style.setProperty('font-style', 'italic', 'important');
        });
      } catch(e) {}
    }
    restyleAttachmentAlert();
    italicizeAttachmentTitle();
    var mo2 = new MutationObserver(restyleAttachmentAlert);
    mo2.observe(document.getElementById('daform') || document.body, { childList: true, subtree: true });
    var mo3 = new MutationObserver(italicizeAttachmentTitle);
    mo3.observe(document.getElementById('daform') || document.body, { childList: true, subtree: true });
    
    // Force custom favicon on every page
    function setCustomFavicon() {
      try {
        // Remove any existing favicon links
        var existingFavicons = document.querySelectorAll('link[rel*="icon"]');
        existingFavicons.forEach(function(link) {
          link.remove();
        });
        
        // Try multiple favicon paths
        var faviconPaths = [
          '/packagestatic/docassemble.playground1Aug25Optimization/favicon.ico',
          '/packagestatic/docassemble.playground1Aug25Optimization/logo.png',
          '/static/docassemble.playground1Aug25Optimization/favicon.ico',
          '/static/docassemble.playground1Aug25Optimization/logo.png'
        ];
        
        faviconPaths.forEach(function(path) {
          var link = document.createElement('link');
          link.rel = 'icon';
          link.type = path.endsWith('.ico') ? 'image/x-icon' : 'image/png';
          link.href = path + '?v=' + Date.now();
          document.head.appendChild(link);
          
          // Also add shortcut icon
          var shortcutLink = document.createElement('link');
          shortcutLink.rel = 'shortcut icon';
          shortcutLink.type = path.endsWith('.ico') ? 'image/x-icon' : 'image/png';
          shortcutLink.href = path + '?v=' + Date.now();
          document.head.appendChild(shortcutLink);
        });
        
        console.log('Favicon set with multiple paths');
      } catch(e) {
        console.log('Favicon error:', e);
      }
    }
    
    // Set favicon immediately and on page load
    setCustomFavicon();
    document.addEventListener('DOMContentLoaded', setCustomFavicon);
    window.addEventListener('load', setCustomFavicon);
    
    // Simple favicon override
    setTimeout(function() {
      try {
        // Add favicon with the working path
        var workingPath = 'packagestatic/docassemble.playground1Aug25Optimization/logo.png';
        
        var link = document.createElement('link');
        link.rel = 'icon';
        link.type = 'image/png';
        link.href = workingPath + '?v=' + Date.now();
        document.head.appendChild(link);
        
        console.log('Simple favicon set to:', workingPath);
      } catch(e) {
        console.log('Simple favicon error:', e);
      }
    }, 100);
    
    // Email functionality
    window.sendEmailReport = function() {
      console.log('sendEmailReport function called');
      var statusDiv = document.getElementById('email-status');
      if (statusDiv) {
        statusDiv.innerHTML = '<div class="alert alert-info">📧 Sending email... Please wait.</div>';
      }
      console.log('Making email request...');
      
      // Make AJAX call to send email
      fetch('/interview?i=docassemble.playground1Sept9AgenticImprovements:main_interview.yml&event=send_email', {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          action: 'send_document_email'
        })
      })
      .then(response => {
        console.log('Email response status:', response.status);
        return response.json();
      })
      .then(data => {
        console.log('Email response data:', data);
        if (statusDiv) {
          if (data.success) {
            statusDiv.innerHTML = '<div class="alert alert-success">✅ Email sent successfully! Check your inbox.</div>';
          } else {
            statusDiv.innerHTML = '<div class="alert alert-danger">❌ Email failed: ' + (data.error || 'Unknown error') + '</div>';
          }
        }
      })
      .catch(error => {
        console.error('Email error:', error);
        if (statusDiv) {
          statusDiv.innerHTML = '<div class="alert alert-danger">❌ Email error: ' + error.message + '</div>';
        }
      });
    };
    
    window.downloadReport = function() {
      // Hide email options and show download
      var emailCard = document.querySelector('.assessment-card');
      if (emailCard) {
        emailCard.style.display = 'none';
      }
    };
  })();
  </script>

# =============================================================================
# RESULTS SCREEN CONFIGURATION
# =============================================================================
# Purpose: Configure results screen behavior and navigation
# Features: Hidden continue button, conditional display, restart functionality

# Remove default buttons entirely on the final screen
hide continue button: "True"
need:
  - comprehensive_variables_defined
  - prepare_help_tokens_for_doc
  - capture_help_tokens_block
  - transfer_main_report_tokens

# Show results as soon as the user has chosen to proceed
show if: defined('proceeded_to_results') and proceeded_to_results
# No continue button on results screen - only Restart/Exit buttons

---
# =============================================================================
# ASSESSMENT CANCELLATION HANDLING
# =============================================================================
# Purpose: Handle user cancellation of the assessment process
# Features: Clear messaging and restart option

id: canceled
question: |
  ## Assessment canceled
  
  You canceled processing. You can restart the interview anytime.

buttons:
  - Restart: restart
